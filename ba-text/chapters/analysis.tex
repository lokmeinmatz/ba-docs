% ---------------------------------------------------
% ----- Chapters of the template
% ----- for Bachelor-, Master thesis and class papers
% ---------------------------------------------------
%  Created by C. Müller-Birn on 2012-08-17, CC-BY-SA 3.0.
%  Freie Universität Berlin, Institute of Computer Science, Human Centered Computing. 
%
% TODO remove 2 - to use auto numbering
\chapter{User research and analysis}
\label{chap:research}


% Due to the limited size of the user group, the goal was not to gain <TODO> with high diversity of their demographics, but to have information saturation from fewer but more valuable insights into peolpe with diffrent workflows.

TODO maybe use this reference?:
\begin{itemize}
  \item \cite{Ross:2016} why companies dont conduct user research
\end{itemize}

Over ten years after the publication of Tomer Sharon's book ''It's our Research'', the list of quotes in the introduction about user research in software companies still feels as relevant as ever.
\\
''Yeah, but this study will delay our launch date.'', ''Yeah, but we can't learn much from only five participants.'', ''Yeah, but research sounds so academic.'' \cite[p. 4]{Sharon:2012mk} are only some of the statements that according to Sharon are often heard in software companies in discussions about \Gls{ux} research.
% TODO "ux" important, but conducting UX research is not used, maybe lik to Ross:2016
\\
The pressure from stakeholders often leads to quick implementation of features and workflows without figuring out the user's needs.
This seems faster in the beginning, but can badly impact the user's acceptance of the product due to cumbersome and slow workflows,
in the worst case it leads to the refusal of the whole product.

To counteract this, it is crucial to conduct user research methods and evaluate the user's needs.
This fact is taken into account in the development of the UI Editor.
A starting point for qualitative user research is to define the goals through the help of the SMART criteria, which provide guidelines and formulated goals during research.
For the project, the SMART criteria were defined as following:
\label{fig:smart}
\begin{itemize}
  \item \textbf{specfic} - improve the workflow of users modifying dynamic resources for Purple Experience
  \item \textbf{measurable} - interviews after testing period concerning working speed, confidence and joy when editing resources, automated user tracking
  \item \textbf{achievable} - research and implementation will mostly be conducted by me, with input from CTO \& product owner, connections to external users through customer service team
  \item \textbf{relevant} - new software platform which reacts quicker, provides more safety regarding errors and is scalable and extensible in the future.
  \item \textbf{time-bound} - the new software should have a feature set enabling productive work, replace the old \textit{Storefront Editor} and be usable by company-internal users until the end of 2022
\end{itemize}
Remembering these points was helpful when the focus on the actual goals was unclear.
\\\\
In addition, Becker (\cite[pp. 37-41]{LearnHCI:2020ys}) states that there are ''three major factors that an HCI designer should consider''.
These three factors were seldom mentioned in publications but are helpful guidelines.
\begin{description}
  \item[Usability Factor] describes the spectrum from ''usable'' to ''unusable'' software. This is determined by the implemented software design features and how they support the user achieve their tasks in the environment provided.
  \item[Accessibility Factor] is high when as many users from different backgrounds can use the software in different environments. 
  It includes access for people with and without disabilities as well as low entry hurdles for new users.
  At first glance this factors doesn't look as important as the other two, especially for specialized applications, but it shouldn't be left out when designing and implementing software.
  \item[Time-On-Task Factor] refers to ''[...] solutions that use up the appropriate of time to solve a problem.'' \Cite[p. 40]{LearnHCI:2020ys}. Obviously, to save users time, a fast system is required.
But it is inevitable that the system, network delay and complexity of computation all have some minimal required time, and users understand that fact. More important is to reduce the perceived lag of interactions and give users feedback if a task takes longer to complete.
\end{description}

\section{Identifying and categorizing users and user groups}
\label{sec:user-groups}
In order to effectively design and implement the UI Editor, it is crucial to understand the needs and preferences of the various users and user groups who will be using the tool.

The first step in the user research process was to identify and categorize them.
Then, detailed information is gained through the observations and interviews and finally in section (\ref{sec:personas}) concrete Personas for the different user groups are built.
\\
It was easy to collect a list of internal and external users for research because Sprylab already had existing users familiar with previous editors and tools from the ecosystem.
A larger group of different users or external customers requires more effort to identify and contact suitable users.
\\
With an overview over current and potential users, the next step was grouping them to understand the characteristics and needs of each user group.
Having a rough set of groups makes it easier to ensure that the UI Editor is tailored to their specific requirements and can be used effectively by all users\footnote{When I refer to ''all users'', I mean the group of users that are expected to work with the tool. There is an expected technical and domain specific base knowledge that the Editor won't cover in it's initial form}.
Also, when choosing who to use as research subjects this list can help getting broad coverage of users with different needs an abilities.
\\
The information of the users leads to the following common factors:
TODO uppercase/lowercase
\begin{description}
  \item[Quantitative usage -] There were users who relied on the tools for most of their work, while others like the external customers accessed the tool a few times a year.
  \item[Common tasks -] I roughly categorized the common tasks into three groups:
  \begin{description}
    \item[Heavy configuration -] Mostly internal devs used the tools to build new apps and websites from scratch, making a wide range of modifications and structural changes.
    \item[Moderate configuration -] Project devs and customer support people copy resources from existing apps and adapt them for new brands, which often includes changing colors and logos, adapting texts or switching authentication flows.
    \item[Small changes -] External customers often only use the tools to exchange some ads, translations or logos, which affects a small set of files.
  \end{description}
  \item[Expertise] -
  \begin{description}
    \item[Technical -] Depending on the area of education and work experience in the web development industry, the expertise about web technologies, languages like CSS and JSON and often also intuition differs between users.
    \item[Domain- and Platform Specific -] There is a lot of vocabulary, functionality of Purple Experience and other systems as well as permutations of configurations that users learn with time.
  \end{description}
  %<kann man bei schlechter software gut erkennen, leute mit viel erfahrung checken sachen, aber ist für neue nicht intutitiv>
\end{description}

% ... more
\section{Qualitative user research}

The existing user base simplified access to subjects for qualitative user research methods, in combination with the advantage of having most users accessable via company-internal communication channels.
Using one or multiple ways of Triangulation \cite[p. 264]{Interactiondesign:2019ys} can strengthen the significance of the research outcome. Limitations of a method or source can be removed by variation of those, resulting in a less distorted picture.
Therefore, metholodical triangulation (using multiple data gathering techniques) as well as triangulation of data (collecting data from different people and different sources) seemed well suited.
Moderated observations combined with interviews proofed to be a good fit for this case study, as they are interaction driven and the observer can react directly on behaviors / emerging topics and steer the process.
This stands in contrast to more passive methods like passive observations or user recording and tracking analysis, where the outcome only depends on the
prepared question / task and the users behavior and which can't adapt to changed circumstances etc. during the application.
\\\\
The chosen structure for the observations and interviews looks as following:

\begin{description}
  \item [Introduction (ca. 5min)] used to explain the circumstances and the goal of the session, how we proceed, which data I will collect and how I will evaluate the data afterwards.
  \item [Moderated Observation (10-15min)] have the observed perform specific tasks in a prepared environment
  \item [Semistructured Interview (ca. 15min)] ask prepared questions as well as open ones and discuss observation situation
\end{description}
Interviews followed observations to discuss issues encountered during the moderated observation and to gain deeper insight into the workflow and potential problems.
\\
To validate the metholodical approach and concrete questions fit the process and can yield desired information, it was important to test the whole process with a small group before scheduling all the other meetings. One of Sprylab's working students agreed to be a test candidate and we went through the tasks and questions I planned, after which he gave me feedback.
Testing the methods and specific questions before conducting them on a broader audience helped to find questions that were ambiguous or lead to a lot of repetition of already known facts.
For the moderated observation tasks the test gave feedback on the difficulty and time the sessions would probably take on average, which tasks needed clearer formulations and which were already sufficient.
\\
In total, I performed the user research with six persons, from which one was a core Purple Experience developer, two were working students from our project department, one was a developer from a different department who had worked with the software some time prior, one Customer Support Manager from our company and one external user from a publisher.
\\
An interesting concept relevant for the choice if subjects from \cite[p. 41]{AboutFace:2014ys} is the Subject Matter Expert (SME), who represents ''authorities on the domain on which the product will operate.'' \cite[p. 41]{AboutFace:2014ys}. Having at least one SME in the process is important as he can give input on technical details and patterns other less involved users might not know. For this case study, the Purple Experience developer represents the SME for the underlying platform the UI Editor os built for.

\subsection{Moderated observation}
\label{subsec:modobs}
I prepared a list of six tasks, with the last two beeing optional depending on the time left and the confidence of the user with the platform I perceived during the beginning.
That way I could present the same first tasks to every interviewee regardless of their level of knowledge, and present the last two tasks if we had enough time left.
Also, it was important that the tasks did not build on each other to prevent the observed person getting stuck because of an earlier mistake.
To me, it was more important to see a variety of tasks getting performed than one task beeing executed without errors. 
\\
In practice, I prepared an example app on our staging system, noted the link down and downloaded the initial state so that I could easily reset the enviornment after each observation.
The six tasks were
\begin{itemize}
  \item Change english app menu entry "Newsstand" to "Home" on all platforms (Web, Android and iOS)
  \item Change the Advertisement banner target on top of the home page to https://google.com 
  \item Change the exnglish text "Latest Issues" on the home page to "Read new Issues"
  \item Change color of "Read new Issues" and "Latest Articles" headers on the home page to the app's primary color.
  \item \textit{(Optional)} Add an dropdown on the home between "Read new Issues" and "Latest Articles"
    \subitem It should show all publications connected to the app
    \subitem It should set an URL parameter ''publication'' to the id when selected
    \subitem Define the reset message as ''All publications''
  \item \textit{(Optional)} Configure the filter of the ''Latest Articles'' list to only show articles from that publication
\end{itemize}

These were constructed in a way that I anticipated some errors so I could see how users tried to figure out what went wrong and fix them.
These cases then also occurred, for example the first task was often only done for one of the three platforms, the ''Latest Issues'' text was not found in the translation files or
the wrong CSS selector was used to recolor the header elements on the home page, leading to more elements beeing recolored.

The optional tasks were presented to four people, of which three solved them at least one of them successfully, only the core developer solved all six tasks completely.
With the consent of the interviewees I recorded their screens during the observation to rewatch specific actions or flows if required.
\\
TODO: noticed workflow patterns that can be improved? like file opening, switching files (quick links \& file tabs)

\subsection{Interview}
\label{subsec:interview}
For the interview, I chose a semistructured interview as the appropriate tool. The interviewer has a list of open and closed questions, through which he can ensure that important topics are coveredw hile also allowing for flexibility to delve deeper into specific issues and ideas that may arise during the interview.
\\
My prepared questions consisted of some closed questions, like how often they interact with the tools, how confident they feel implementing changes or fixing errors, as well as some open questions like to describe step by step what the most recent task was they performed with dynamic resources, how they were onboarded etc.
Then, if I had notices points during the observation, I addressed these points directly, else I directly introduced the open question ''If you dream of the UI builder, <>, how would it look like, which features would you expect and what workflows are most important to you?''.
\\
The semistructured interviews proved to be valuable in providing insight into the experiences and needs of the users. Through the interviews, I was able to gather a lot of new ideas and saw how different features were massively valued differently by the users. Two of the interviewees even provided written lists of their ideas and suggestions which they sent me afterwards. After every interview (wich I also recorded), I filled out a Word document noting basic questions about the person and it's usage of the dynamic resources, important situations from the observation, input from the interview and linked to the recordings in case I needed to rewatch parts of it.
\\\\
Some of the outcomes were:
\begin{itemize}
  \item The way users validate their changes differs widely, some use a preview window in the old editor, some merge changes into the resources and view it on the website or inspect the app's javascript context, some prefer seperate windows, some embedded frames to see code and preview at the same time.
  \item Rearranging or adapting the layout of the tools / editor to match the current screen and browser window size is important, e.g. hide unecessary panes if not used, make the window the user currently works in larger to see more content at once.
  \item Editing multiple files at once (in the old editor one could only edit one file at a time, had to close the current and open the next one).
  \item Colaborative working: Seeing the current state of the resources, e.g. if a live or preview resource is processing, if other users are working in the same app parallel, and having git-like line-by-line diffs of the changes made.
\end{itemize}

Some of these points and their implementations will be covered in chapter \ref*{chap:prototyping} \ref*{chap:impl}.

\section{Quantitaive user research}

In the initial stages of research, the use of questionnaires was considered as a method of gathering information from existing and potential users.
However, as the outcome of the qualitative research was already productive, it proved difficult to design a questionnaire that would provide additional information while adhering to common design rules for questionnaires in terms of length, number of questions, and type of questions.
While prototype questionnaires were created using Microsoft Forms, I found them to be too long, difficult to understand, or not relevant to the development of prototypes.
Therefore, it was decided to rely solely on qualitative research results for prototyping and feedback, along with some automatic tracking implementation (See TODO squeaky ref).
\\
\section{Process and visualize the outcomes of the initial user research phase}

To gain as much value from the raw research data it is crucial to find some methods to process and visualize the outcomes.
A common approach at companies is to write ''tickets'' in their ticket system of choice, in case of Sprylab Jira (\url{https://www.atlassian.com/de/software/jira}).
From my experience, Jira and co. proof valuable once the development process itself started and the state of the tasks must be tracked.
But for discovery and prioritization of features, scrolling through lists of tickets with different priority levels, where then the tickets in the lists are sorted differently again, does not allow a good overview of effort and benefit of each feature.
Thus, based on the condensed notes from the observations and interviews I tried out two alternative ways to figure out different features and how they can be prioritized.

\subsection{2x2 Opportunity Matrix}

This two-dimensional vizualization of a set of proposed features prooved helpful when prioritizing tasks with other stakeholders,
as it shows the (approximated) cost of implementation as well as the value the feature can have for users.

The matrix I used is a slightly modified adaption from \cite[p. 181]{LearnHCI:2020ys}, replaced the term ''idea originality'' on the x-axis with ''Value''.

\begin{figure}[h!]
	\centering
  \includegraphics[width=\textwidth]{pics/feature_cost_matrix.excalidraw.png}
	\caption{2x2 Opportunity Matrix during the early phases of development TODO: redo chart in drawio}
	\label{fig:opportunitymatrix}
\end{figure}


\section{Building Personas}
\label{sec:personas}

TODO: move to appendix?

Personas are descriptions of fictional users of the product, incorporating assumptions and optinally data for a user group.
They aim to give developers and designers more context and depict real potential users, which makes it easier for a developer to empathize with the user.
The following three Role-based Personas are derived from \ref{sec:user-groups} and the outcomes of the interviews, based on the description of Personas in \cite[pp. 403-405]{Interactiondesign:2019ys}
